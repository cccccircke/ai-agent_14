{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a3fc13508644456fa81205f6a5588324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_026a48b9547d4d2497bc6d36972674a9",
              "IPY_MODEL_19fd7d58886d482e81f5b2de2339a1ea",
              "IPY_MODEL_f389ca54272e4168b7abca1841d2277c"
            ],
            "layout": "IPY_MODEL_a7ccda929071407782caf738614afb40"
          }
        },
        "026a48b9547d4d2497bc6d36972674a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a956bd6d284c93be966bd78d7ea7a1",
            "placeholder": "​",
            "style": "IPY_MODEL_fee4b00dc6e54b6699e5b0fd81960e7f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "19fd7d58886d482e81f5b2de2339a1ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e553a2c9bc4b4403afbf8498068d46bb",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_088dad87ba28470fbc29226808aeaf86",
            "value": 3
          }
        },
        "f389ca54272e4168b7abca1841d2277c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76fb939b2e73434ba4fa291c3081143c",
            "placeholder": "​",
            "style": "IPY_MODEL_941cb87758054726a983abc46bb5ea12",
            "value": " 3/3 [01:10&lt;00:00, 23.00s/it]"
          }
        },
        "a7ccda929071407782caf738614afb40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a956bd6d284c93be966bd78d7ea7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee4b00dc6e54b6699e5b0fd81960e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e553a2c9bc4b4403afbf8498068d46bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "088dad87ba28470fbc29226808aeaf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76fb939b2e73434ba4fa291c3081143c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941cb87758054726a983abc46bb5ea12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to create a new folder `outfits` and upload all the files there first after connecting to a runtime\n"
      ],
      "metadata": {
        "id": "wxhoVg4QXtJV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km78MgWkW-H7",
        "outputId": "20f4685d-1eff-468c-cd09-54298f02c395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ],
      "source": [
        "# 1. Install necessary libraries\n",
        "# transformers: For LLaVA model\n",
        "# accelerate/bitsandbytes: For 4-bit memory efficient loading on GPU\n",
        "# tqdm: For the progress bar\n",
        "!pip install transformers accelerate bitsandbytes tqdm Pillow\n",
        "\n",
        "# 2. Imports\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_outfit_response(response):\n",
        "    \"\"\"\n",
        "    Parse the LLaVA response into structured outfit data.\n",
        "    \"\"\"\n",
        "    lines = response.split('\\n')\n",
        "    # Updated default keys to include \"type\"\n",
        "    outfit_data = {\n",
        "        \"type\": \"Unknown\", \"category\": \"Unknown\", \"subcategory\": \"Unknown\",\n",
        "        \"color_primary\": \"Unknown\", \"color_secondary\": \"N/A\", \"pattern\": \"Unknown\",\n",
        "        \"material\": \"Unknown\", \"sleeve_length\": \"N/A\", \"length\": \"Unknown\",\n",
        "        \"style_aesthetic\": \"Unknown\", \"fit_silhouette\": \"Unknown\",\n",
        "        \"complete_description\": \"\"\n",
        "    }\n",
        "\n",
        "    # Parse key-value pairs from response\n",
        "    for line in lines:\n",
        "        if ':' in line:\n",
        "            key, value = line.split(':', 1)\n",
        "            key = key.strip().upper()\n",
        "            value = value.strip()\n",
        "\n",
        "            # Map to outfit data keys (now includes TYPE)\n",
        "            if key == \"TYPE\": outfit_data[\"type\"] = value\n",
        "            elif key == \"CATEGORY\": outfit_data[\"category\"] = value\n",
        "            elif key == \"SUBCATEGORY\": outfit_data[\"subcategory\"] = value\n",
        "            elif key == \"COLOR_PRIMARY\": outfit_data[\"color_primary\"] = value\n",
        "            elif key == \"COLOR_SECONDARY\": outfit_data[\"color_secondary\"] = value if value else \"N/A\"\n",
        "            elif key == \"PATTERN\": outfit_data[\"pattern\"] = value\n",
        "            elif key == \"MATERIAL\": outfit_data[\"material\"] = value\n",
        "            elif key == \"SLEEVE_LENGTH\": outfit_data[\"sleeve_length\"] = value if value else \"N/A\"\n",
        "            elif key == \"LENGTH\": outfit_data[\"length\"] = value\n",
        "            elif key == \"STYLE_AESTHETIC\": outfit_data[\"style_aesthetic\"] = value\n",
        "            elif key == \"FIT_SILHOUETTE\": outfit_data[\"fit_silhouette\"] = value\n",
        "            elif key == \"COMPLETE_DESCRIPTION\": outfit_data[\"complete_description\"] = value\n",
        "\n",
        "    return outfit_data\n",
        "\n",
        "\n",
        "def generate_outfit_descriptions(folder_path, output_path):\n",
        "    \"\"\"\n",
        "    Generate detailed outfit descriptions using LLaVA Vision Language Model.\n",
        "    \"\"\"\n",
        "    # 1. Device and Quantization Setup (CRITICAL FOR COLAB GPU)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "    )\n",
        "\n",
        "    # 2. Load LLaVA model\n",
        "    print(\"Loading LLaVA Vision Language Model (4-bit)...\")\n",
        "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "    model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\" # Automatically maps to GPU\n",
        "    )\n",
        "\n",
        "    # 3. Get all image files and sort them\n",
        "    folder = Path(folder_path)\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp', '.gif', '.tiff'}\n",
        "\n",
        "    image_files = [file for file in folder.iterdir() if file.is_file() and file.suffix.lower() in image_extensions]\n",
        "\n",
        "    def sort_key(f):\n",
        "        try:\n",
        "            return int(f.stem)\n",
        "        except ValueError:\n",
        "            return float('inf')\n",
        "\n",
        "    image_files.sort(key=sort_key)\n",
        "    print(f\"Found {len(image_files)} images\")\n",
        "\n",
        "    # 4. Prompt for detailed outfit analysis (Uses the same prompt text from your file)\n",
        "    analysis_prompt = \"\"\"Analyze this outfit image in detail and provide structured information about the clothing item/outfit shown.\n",
        "\n",
        "    Please identify and describe:\n",
        "    1. Type (Upper for tops/shirts/jackets, Lower for pants/skirts/shorts, Dress for one-piece outfits/dresses)\n",
        "    2. Category (e.g., Outerwear, Top, Bottom, Dress, Accessories, Footwear)\n",
        "    3. Subcategory (e.g., Overcoat, T-shirt, Jeans, Sneakers)\n",
        "    4. Primary color\n",
        "    5. Secondary colors (if any)\n",
        "    6. Pattern (Solid, Striped, Plaid, Floral, etc.)\n",
        "    7. Material/Fabric type (Cotton, Wool, Silk, Leather, etc.)\n",
        "    8. Sleeve length (for tops/outerwear)\n",
        "    9. Overall length/fit\n",
        "    10. Style aesthetic (Business Casual, Streetwear, Minimalist, Vintage, etc.)\n",
        "    11. Fit/Silhouette (Fitted, Relaxed, Oversized, etc.)\n",
        "    12. A complete 2-3 sentence description of the outfit\n",
        "\n",
        "    Format your response as follows:\n",
        "    TYPE: [Upper/Lower/Dress]\n",
        "    CATEGORY: [category]\n",
        "    SUBCATEGORY: [subcategory]\n",
        "    COLOR_PRIMARY: [primary color]\n",
        "    COLOR_SECONDARY: [secondary colors or N/A]\n",
        "    PATTERN: [pattern type]\n",
        "    MATERIAL: [material/fabric]\n",
        "    SLEEVE_LENGTH: [sleeve length or N/A]\n",
        "    LENGTH: [length/fit description]\n",
        "    STYLE_AESTHETIC: [aesthetic style]\n",
        "    FIT_SILHOUETTE: [fit type]\n",
        "    COMPLETE_DESCRIPTION: [2-3 sentence description]\"\"\"\n",
        "\n",
        "    # 5. Generate descriptions\n",
        "    outfit_descriptions = {}\n",
        "\n",
        "    for image_path in tqdm.tqdm(image_files):\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            prompt_template = f\"USER: <image>\\n{analysis_prompt}\\nASSISTANT:\"\n",
        "\n",
        "            inputs = processor(images=image, text=prompt_template, return_tensors=\"pt\")\n",
        "\n",
        "            # Move inputs to the GPU\n",
        "            inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "\n",
        "            # Generate response\n",
        "            with torch.no_grad():\n",
        "                output_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=300,\n",
        "                    do_sample=False, # Use greedy search for structured output\n",
        "                )\n",
        "\n",
        "            response = processor.decode(output_ids[0], skip_special_tokens=True)\n",
        "            response_text = response.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "            # 6. Parse and Store\n",
        "            outfit_data = parse_outfit_response(response_text)\n",
        "            outfit_descriptions[image_path.name] = outfit_data\n",
        "\n",
        "        except Exception as e:\n",
        "            # 7. Error Handling (Stores error message, does NOT overwrite good data)\n",
        "            outfit_descriptions[image_path.name] = {\n",
        "                \"type\": \"Execution Error\",\n",
        "                \"category\": \"Unknown\",\n",
        "                \"complete_description\": f\"Error: {str(e)}\",\n",
        "                \"error\": True\n",
        "            }\n",
        "\n",
        "    # 8. Save to JSON file\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(outfit_descriptions, f, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved outfit descriptions to: {output_path}\")\n",
        "    print(f\"Successfully processed {len(outfit_descriptions)} images\")\n",
        "\n",
        "    # 9. Print sample entries\n",
        "    print(f\"\\nSample descriptions (first 2 items):\")\n",
        "    for filename, data in list(outfit_descriptions.items())[:2]:\n",
        "        print(f\"\\n{filename}:\")\n",
        "        print(f\"  Type: {data.get('type', 'Unknown')}\")\n",
        "        print(f\"  Style: {data.get('style_aesthetic', 'Unknown')}\")"
      ],
      "metadata": {
        "id": "velkxHviXWND"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure your 'outfits' folder with images is uploaded to the Colab session!\n",
        "\n",
        "outfits_folder = \"outfits\"\n",
        "output_file = \"outfit_descriptions.json\"\n",
        "\n",
        "if not os.path.exists(outfits_folder):\n",
        "    print(f\"ERROR: Folder '{outfits_folder}' not found! Please upload your images.\")\n",
        "else:\n",
        "    generate_outfit_descriptions(outfits_folder, output_file)\n",
        "    print(\"\\n--- Processing Complete ---\")\n",
        "    print(f\"Download '{output_file}' from the Colab file browser now!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "a3fc13508644456fa81205f6a5588324",
            "026a48b9547d4d2497bc6d36972674a9",
            "19fd7d58886d482e81f5b2de2339a1ea",
            "f389ca54272e4168b7abca1841d2277c",
            "a7ccda929071407782caf738614afb40",
            "32a956bd6d284c93be966bd78d7ea7a1",
            "fee4b00dc6e54b6699e5b0fd81960e7f",
            "e553a2c9bc4b4403afbf8498068d46bb",
            "088dad87ba28470fbc29226808aeaf86",
            "76fb939b2e73434ba4fa291c3081143c",
            "941cb87758054726a983abc46bb5ea12"
          ]
        },
        "id": "UvZyNfRRXacT",
        "outputId": "4c596702-7e74-4c42-af49-ee319a544835"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading LLaVA Vision Language Model (4-bit)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3fc13508644456fa81205f6a5588324"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 58 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [07:36<00:00,  7.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved outfit descriptions to: outfit_descriptions.json\n",
            "Successfully processed 58 images\n",
            "\n",
            "Sample descriptions (first 2 items):\n",
            "\n",
            "1.jpg:\n",
            "  Type: Dress\n",
            "  Style: Business Casual\n",
            "\n",
            "2.jpg:\n",
            "  Type: Dress\n",
            "  Style: Casual\n",
            "\n",
            "--- Processing Complete ---\n",
            "Download 'outfit_descriptions.json' from the Colab file browser now!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}